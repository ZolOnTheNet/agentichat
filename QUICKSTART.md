# agentichat - Quick Start

## Installation Rapide

```bash
# 1. CrÃ©er l'environnement virtuel
uv venv

# 2. Installer le projet
uv pip install -e .

# Configuration automatiquement crÃ©Ã©e dans : ~/.agentichat/config.yaml
```

## PrÃ©requis

**Ollama doit Ãªtre installÃ© et lancÃ© :**

```bash
# VÃ©rifier qu'Ollama fonctionne
curl http://localhost:11434/api/tags

# Si besoin, installer Ollama : https://ollama.ai/
# Puis lancer le serveur
ollama serve

# TÃ©lÃ©charger un modÃ¨le si nÃ©cessaire
ollama pull qwen2.5:3b
```

## Utilisation

### Lancer le chat interactif

```bash
.venv/bin/agentichat
```

### Exemples de conversation

**Message simple :**
```
> Bonjour, peux-tu m'expliquer ce qu'est Python en une phrase ?
[Le LLM rÃ©pond...]
```

**Message multi-ligne (Shift+Enter) :**
```
> Ã‰cris-moi un haiku
... sur la programmation
... [Appuyer sur Enter pour envoyer]

[Le LLM rÃ©pond...]
```

**Utiliser l'historique :**
- Appuyez sur `â†‘` pour voir le message prÃ©cÃ©dent
- Appuyez sur `â†“` pour revenir au message suivant
- Votre brouillon est prÃ©servÃ© pendant la navigation

### Commandes disponibles

```
/help       Afficher l'aide
/clear      RÃ©initialiser la conversation
/quit       Quitter (ou Ctrl+D)
```

### Raccourcis clavier

| Raccourci | Action |
|-----------|--------|
| `Enter` | Envoyer le message |
| `Shift+Enter` | Nouvelle ligne |
| `â†‘` / `â†“` | Naviguer dans l'historique |
| `Ctrl+C` | Annuler la saisie |
| `Ctrl+D` | Quitter |

## VÃ©rifier la configuration

```bash
.venv/bin/agentichat config show
```

Affiche :
- Backend actif (ollama)
- ModÃ¨le utilisÃ© (qwen2.5:3b)
- URL du serveur
- ParamÃ¨tres (timeout, max_tokens, etc.)

## Tests Automatiques

Valider que tout fonctionne :

```bash
.venv/bin/python test_backend.py
```

RÃ©sultat attendu :
```
=== Test Backend Ollama ===

1. Test health check...
   âœ“ Serveur Ollama accessible

2. Liste des modÃ¨les disponibles...
   âœ“ 1 modÃ¨le(s) trouvÃ©(s): qwen2.5:3b

3. Test chat simple (rÃ©ponse complÃ¨te)...
   âœ“ RÃ©ponse : Paris

4. Test chat streaming...
   âœ“ Streaming fonctionnel

=== Tous les tests sont passÃ©s ! ===
```

## RÃ©solution de ProblÃ¨mes

### Erreur : "Impossible de se connecter"

```
Erreur: Impossible de se connecter Ã  http://localhost:11434
```

**Solution :**
1. VÃ©rifier qu'Ollama est lancÃ© : `ollama serve`
2. VÃ©rifier qu'un modÃ¨le est disponible : `ollama list`
3. Si besoin, tÃ©lÃ©charger un modÃ¨le : `ollama pull qwen2.5:3b`

### Modifier le modÃ¨le

Ã‰diter `~/.agentichat/config.yaml` :

```yaml
backends:
  ollama:
    model: mistral:latest  # Changer ici
```

### Utiliser un serveur Ollama distant

Dans `~/.agentichat/config.yaml` :

```yaml
backends:
  ollama:
    url: http://192.168.1.100:11434  # IP du serveur
    model: qwen2.5:3b
```

## Documentation ComplÃ¨te

- `README.md` - Vue d'ensemble
- `PHASE1_COMPLETE.md` - RÃ©capitulatif dÃ©taillÃ© Phase 1
- `PHASE1_TESTING.md` - Guide de tests complet
- `docs/` - Documentation de conception

## Prochaines Phases

**Phase 2 (en cours de dÃ©veloppement) :**
- Tools systÃ¨me (read_file, write_file, shell_exec)
- Boucle agentique
- SystÃ¨me de confirmation Y/N/A

**Exemple futur :**
```
> CrÃ©e un fichier hello.py avec un Hello World et exÃ©cute-le

[Le LLM crÃ©era le fichier, demandera confirmation, puis l'exÃ©cutera]
```

---

**Bon chat ! ğŸš€**
