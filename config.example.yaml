# Configuration exemple pour agentichat
# Copier ce fichier vers ~/.agentichat/config.yaml ou .agentichat/config.yaml (workspace local)

# Backend par d√©faut √† utiliser
# Options : ollama, albert
default_backend: ollama  # Changez en 'albert' pour utiliser l'API Albert

# Backends disponibles
backends:
  # Ollama local
  ollama:
    type: ollama
    url: http://localhost:11434
    model: llama3:8b
    timeout: 300  # 5 minutes pour les requ√™tes complexes avec tools
    max_tokens: 4096
    temperature: 0.7

  # Albert API (Etalab - Service public fran√ßais)
  # D√©commentez et configurez pour utiliser Albert
  albert:
    type: albert
    url: https://albert.api.etalab.gouv.fr
    model: mistralai/Mistral-Small-3.2-24B-Instruct-2506  # Voir MODEL_ALBERT.md
    api_key: YOUR_API_KEY_HERE  # REQUIS - Obtenir sur https://albert.api.etalab.gouv.fr
    timeout: 180  # Timeout en secondes (augment√© pour gros mod√®les)
    max_tokens: 4096
    temperature: 0.7
    # max_parallel_tools: 1  # Limite de tools parall√®les (d√©tect√© automatiquement si besoin)
    #
    # ‚ö° Features exclusives Albert (4 tools suppl√©mentaires) :
    # - albert_search      : Recherche s√©mantique dans vos documents index√©s
    # - albert_ocr         : Extraction de texte depuis images/PDF (vision models)
    # - albert_transcription : Conversion audio ‚Üí texte (MP3, WAV, etc.)
    # - albert_embeddings  : Cr√©ation de vecteurs pour recherche s√©mantique
    #
    # üí° Pour utiliser Albert :
    # 1. Cr√©er un compte sur https://albert.api.etalab.gouv.fr
    # 2. G√©n√©rer une cl√© API
    # 3. Remplacer YOUR_API_KEY_HERE par votre cl√©
    # 4. Changer default_backend: albert (ligne 5)

  # Ollama distant (exemple)
  # ollama_remote:
  #   type: ollama
  #   url: http://192.168.1.100:11434
  #   model: mistral:latest
  #   timeout: 60
  #   max_tokens: 8192
  #   temperature: 0.5

  # OpenAI (Phase 4+, non encore impl√©ment√©)
  # openai:
  #   type: openai
  #   url: https://api.openai.com/v1
  #   model: gpt-4
  #   api_key: ${OPENAI_API_KEY}
  #   timeout: 60
  #   max_tokens: 4096
  #   temperature: 0.7

  # vLLM (Phase 4+, non encore impl√©ment√©)
  # vllm:
  #   type: vllm
  #   url: http://localhost:8000
  #   model: meta-llama/Llama-3-8b-chat-hf
  #   timeout: 30
  #   max_tokens: 4096
  #   temperature: 0.7

# Configuration du sandbox de s√©curit√©
sandbox:
  # Taille maximale d'un fichier (en octets)
  max_file_size: 1000000  # 1 MB

  # Chemins bloqu√©s (acc√®s refus√© pour s√©curit√©)
  blocked_paths:
    - "**/.env"
    - "**/*.key"
    - "**/*.pem"
    - "**/id_rsa"
    - "**/credentials.json"
    - "**/.ssh/*"

  # R√©pertoires ignor√©s lors des recherches r√©cursives (list_files, search_text, glob_search)
  # Am√©liore les performances en √©vitant de parcourir des milliers de fichiers inutiles
  # Utilisez include_ignored=True dans les tools pour forcer l'inclusion
  # Voir: /help sandbox
  ignored_paths:
    # Environnements virtuels Python
    - "**/.venv/**"
    - "**/venv/**"
    - "**/env/**"
    - "**/.virtualenv/**"
    # D√©pendances Node.js
    - "**/node_modules/**"
    # Contr√¥le de version
    - "**/.git/**"
    # Caches Python
    - "**/__pycache__/**"
    - "**/.pytest_cache/**"
    - "**/.mypy_cache/**"
    - "**/.ruff_cache/**"
    # Build artifacts
    - "**/build/**"
    - "**/dist/**"
    - "**/*.egg-info/**"
    # IDEs
    - "**/.vscode/**"
    - "**/.idea/**"
    # Autres
    - "**/.DS_Store"
    # Ajoutez vos patterns personnalis√©s ici :
    # - "**/mes-donnees-test/**"
    # - "**/tmp/**"

  # Commandes autoris√©es (null = toutes autoris√©es)
  # allowed_commands:
  #   - git
  #   - npm
  #   - pytest
  #   - ruff

# Configuration des confirmations utilisateur
confirmations:
  # Confirmer les op√©rations d'√©criture/suppression de fichiers
  text_operations: true

  # Confirmer les commandes shell
  shell_commands: true

# R√©pertoire de donn√©es (historique, cache, etc.)
# Par d√©faut: ~/.agentichat/
# data_dir: ~/.agentichat

# Nombre maximum d'it√©rations de la boucle agentique
max_iterations: 10

# Configuration du proxy daemon (Phase 1+)
proxy_port: 5157
proxy_host: 127.0.0.1
